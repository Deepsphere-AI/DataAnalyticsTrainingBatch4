{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Deepsphere-AI/DataAnalyticsTrainingBatch4/blob/main/Python/TeachingFiles/DSSAI_LVA_EDA_Examples.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "## Pandas DataFrame aggregate() Method\n",
        "## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "import pandas as pd\n",
        "import warnings as wr\n",
        "wr.filterwarnings('ignore')\n",
        "data = {\n",
        "  \"KEY1\": [50, 40, 30],\n",
        "  \"KEY2\": [300, 1112, 42]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "#df.shape\n",
        "#print(df)\n",
        "#vAR_df = df.aggregate([\"sum\"])\n",
        "#vAR_df = df.aggregate([\"min\"])\n",
        "#vAR_df = df.aggregate([\"max\"])\n",
        "print(df.mean(skipna = False)) # Skip NaN values , or Missing Values\n",
        "print(vAR_df)"
      ],
      "metadata": {
        "id": "g0V9Nj6bnNuR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "## skipna=True and skipna=False\n",
        "## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "import pandas as pd\n",
        "import warnings as wr\n",
        "wr.filterwarnings('ignore')\n",
        "\n",
        "# create a sample dataframe with NaN values\n",
        "df = pd.DataFrame({\n",
        "    'KEY1': [1, 2, 3, 4, 5, 6],\n",
        "    'KEY2': [7, 8, 9, 10, 11, 12],\n",
        "    'KEY3': [13, 14, None, 16, 17, 18],\n",
        "    'KEY4': [19, None, 21, 22, 23, 24],\n",
        "    'KEY5': [25, 26, 27, 28, None, 30]\n",
        "})\n",
        "#print(df.shape)\n",
        "# calculate the average of columns KEY1, KEY2, and KEY3, ignoring NaN values\n",
        "avg_True = df[['KEY1', 'KEY2', 'KEY3']].mean(skipna=True)\n",
        "avg_False = df[['KEY1', 'KEY2', 'KEY3']].mean(skipna=False)\n",
        "\n",
        "#print the result\n",
        "print(avg_True)\n",
        "print(\"----------------------\\n\")\n",
        "print(avg_False)"
      ],
      "metadata": {
        "id": "Sc1YZ-e4R0y5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "## nanmean() with NaN values\n",
        "## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "import numpy as np\n",
        "import warnings as wr\n",
        "wr.filterwarnings('ignore')\n",
        "# create a sample numpy array with NaN values\n",
        "df = pd.DataFrame({\n",
        "    'KEY1': [1, 2, 3, 4, 5, 6],\n",
        "    'KEY2': [7, 8, 9, 10, 11, 12],\n",
        "    'KEY3': [13, 14, None, 16, 17, 18],\n",
        "    'KEY4': [19, None, 21, 22, 23, 24],\n",
        "    'KEY5': [25, 26, 27, 28, None, 30]\n",
        "})\n",
        "# calculate the average of columns 0, 1, and 2, ignoring NaN values\n",
        "avg = np.nanmean(df[['KEY1', 'KEY2', 'KEY3']])\n",
        "avg = np.nanmean(df[['KEY1', 'KEY2', 'KEY3']], axis=0)\n",
        "avg = np.nanmean(df[['KEY1', 'KEY2', 'KEY3']], axis=1)\n",
        "\n",
        "# print the result\n",
        "print(avg)\n"
      ],
      "metadata": {
        "id": "KsAUHJyhWBMb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "## nanmean() - Custom Aggregate Function\n",
        "## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "import numpy as np\n",
        "import warnings as wr\n",
        "wr.filterwarnings('ignore')\n",
        "# create a sample numpy array with NaN values\n",
        "df = pd.DataFrame({\n",
        "    'KEY1': [1, 2, 3, 4, 5, 6],\n",
        "    'KEY2': [7, 8, 9, 10, 11, 12],\n",
        "    'KEY3': [13, 14, None, 16, 17, 18],\n",
        "    'KEY4': [19, None, 21, 22, 23, 24],\n",
        "    'KEY5': [25, 26, 27, 28, None, 30]\n",
        "})\n",
        "\n",
        "# Custom Aggregate Function\n",
        "def custom_mean(column):\n",
        "    values = [val for val in column if not np.isnan(val)]\n",
        "    return sum(values) / len(values) if len(values) > 0 else np.nan\n",
        "\n",
        "# Calling Cusom Function\n",
        "avg = df.apply(custom_mean, axis=1)\n",
        "print(avg)"
      ],
      "metadata": {
        "id": "DxoBevgOYokf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "## Pandas DataFrame aggregate() Method -  SUM on a Selected Column\n",
        "## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "import pandas as pd\n",
        "import warnings as wr\n",
        "wr.filterwarnings('ignore')\n",
        "vAR_Training = {\n",
        "    'COURSES':[\"Spark\",\"PySpark\",\"Hadoop\",\"Python\",\"PySpark\",\"Spark\"],\n",
        "    'COURSES-FEE' :[20000,25000,26000,22000,24000,35000],\n",
        "    'COURSES-DURATION':['30day','40days','35days','40days','60days','60days'],\n",
        "    'COURSES-DSICOUNT':[1000,2300,1200,2500,2000,2000]\n",
        "              }\n",
        "df = pd.DataFrame(vAR_Training)\n",
        "print(\"Create DataFrame:\\n \\n\", df)\n",
        "\n",
        "# Using Aggregate Function on DataFrame\n",
        "vAR_sum = df[['COURSES-FEE','COURSES-DSICOUNT']].aggregate('min')\n",
        "print(vAR_sum)\n"
      ],
      "metadata": {
        "id": "jhvYy2I5oaM4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "## Pandas DataFrame aggregate() Method # Use DataFrame.groupby() Function\n",
        "## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "import pandas as pd\n",
        "vAR_Training  = {\n",
        "    'COURSES':[\"Spark\",\"PySpark\",\"Hadoop\",\"Python\",\"PySpark\",\"Spark\"],\n",
        "    'COURSES-FEE' :[20000,25000,26000,22000,24000,35000],\n",
        "    'COURSES-DURATION':['30day','40days','35days','40days','60days','60days'],\n",
        "    'COURSES-DSICOUNT':[1000,2300,1200,2500,2000,2000]\n",
        "              }\n",
        "df = pd.DataFrame(vAR_Training)\n",
        "\n",
        "vAR_sum = df.groupby('COURSES')['COURSES-FEE','COURSES-DSICOUNT'].aggregate('sum')\n",
        "print(vAR_sum)"
      ],
      "metadata": {
        "id": "X_mb8yN3ppWN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "## Pandas DataFrame aggregate() Method # Groupby multiple columns & multiple aggregations\n",
        "## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "import pandas as pd\n",
        "vAR_Training  = {\n",
        "    'COURSES':[\"Spark\",\"PySpark\",\"Hadoop\",\"Python\",\"PySpark\",\"Spark\"],\n",
        "    'COURSES-FEE' :[20000,25000,26000,22000,24000,35000],\n",
        "    'COURSES-DURATION':['30day','40days','35days','40days','60days','60days'],\n",
        "    'COURSES-DSICOUNT':[1000,2300,1200,2500,2000,2000]\n",
        "              }\n",
        "df = pd.DataFrame(vAR_Training)\n",
        "# Groupby multiple columns & multiple aggregations\n",
        "result = df.groupby('COURSES').aggregate({'COURSES-DURATION':'count','COURSES-FEE':['min','max']})\n",
        "print(result)"
      ],
      "metadata": {
        "id": "Y7eCOJJ2JXck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "## Pandas DataFrame aggregate() Method # Groupby multiple columns & multiple aggregations\n",
        "## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "import pandas as pd\n",
        "vAR_Training  = {\n",
        "    'COURSES':[\"Spark\",\"PySpark\",\"Hadoop\",\"Python\",\"PySpark\",\"Spark\"],\n",
        "    'COURSES-FEE' :[20000,25000,26000,22000,24000,35000],\n",
        "    'COURSES-DURATION':['30day','40days','35days','40days','60days','60days'],\n",
        "    'COURSES-DSICOUNT':[1000,2300,1200,2500,2000,2000]\n",
        "              }\n",
        "df = pd.DataFrame(vAR_Training)\n",
        "\n",
        "print(\"# Using Aggregate Functions on Entire DataFrame \\n\")\n",
        "result = df[['COURSES-FEE','COURSES-DSICOUNT']].aggregate('sum')\n",
        "print(result)\n",
        "print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ \\n\")\n",
        "\n",
        "\n",
        "print(\"# Use DataFrame.group() Function \\n\")\n",
        "result = df.groupby('COURSES')['COURSES-FEE','COURSES-DSICOUNT'].aggregate('sum')\n",
        "print(result)\n",
        "print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ \\n\")\n",
        "\n",
        "\n",
        "print(\"# Using Aggregate Function on Series \\n\")\n",
        "value = df['COURSES-FEE'].aggregate('sum')\n",
        "print(value)\n",
        "print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ \\n\")\n",
        "\n",
        "\n",
        "print(\"# Using groupby() and aggreaget() \\n\")\n",
        "result = df.groupby('COURSES').aggregate('sum')\n",
        "print(result)\n",
        "print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ \\n\")\n",
        "\n",
        "\n",
        "print(\"# Using groupby() and aggreaget() \\n\")\n",
        "result = df.groupby('COURSES')['COURSES-FEE','COURSES-DSICOUNT'].aggregate('sum')\n",
        "print(result)\n",
        "print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ \\n\")\n",
        "\n",
        "\n",
        "print(\"# Alternate way \\n\")\n",
        "result = df[['COURSES','COURSES-FEE','COURSES-DSICOUNT']].groupby('COURSES').aggregate('sum')\n",
        "print(result)\n",
        "print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ \\n\")\n",
        "\n",
        "\n",
        "print(\"# Directly using sum() function \\n\")\n",
        "\n",
        "result = df.groupby('COURSES').sum()\n",
        "print(result)\n",
        "print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ \\n\")\n",
        "\n",
        "\n",
        "print(\" # Groupby & multiple aggregations \\n\")\n",
        "result = df.groupby('COURSES')['COURSES-FEE'].aggregate(['min','max'])\n",
        "print(result)\n",
        "print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ \\n\")\n",
        "\n",
        "\n",
        "print(\" # Groupby multiple columns & multiple aggregations \\n\")\n",
        "result = df.groupby('COURSES').aggregate({'COURSES-DSICOUNT':'count','COURSES-FEE':['min','max']})\n",
        "print(result)\n",
        "print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ \\n\")"
      ],
      "metadata": {
        "id": "WIYXLkqtMKMn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ljn8vH8LYpg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0a07093-67e9-420a-b387-36588ca94c54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 75 entries, 0 to 74\n",
            "Data columns (total 12 columns):\n",
            " #   Column          Non-Null Count  Dtype \n",
            "---  ------          --------------  ----- \n",
            " 0   EMPLOYEE_ID     75 non-null     int64 \n",
            " 1   FIRST_NAME      75 non-null     object\n",
            " 2   LAST_NAME       75 non-null     object\n",
            " 3   GENDER          75 non-null     object\n",
            " 4   EMAIL           75 non-null     object\n",
            " 5   PHONE_NUMBER    75 non-null     object\n",
            " 6   HIRE_DATE       75 non-null     object\n",
            " 7   JOB_ID          75 non-null     object\n",
            " 8   SALARY          75 non-null     int64 \n",
            " 9   COMMISSION_PCT  75 non-null     object\n",
            " 10  MANAGER_ID      75 non-null     object\n",
            " 11  DEPARTMENT_ID   75 non-null     int64 \n",
            "dtypes: int64(3), object(9)\n",
            "memory usage: 7.2+ KB\n"
          ]
        }
      ],
      "source": [
        "## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "## EDA Examples - Simple\n",
        "## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "\n",
        "#Load the required libraries\n",
        "import pandas as pd\n",
        "\n",
        "#Load the data\n",
        "df = pd.read_csv('/content/sample_data/UNIT3LVADSAIEMPDATAv1.csv')\n",
        "#View the data\n",
        "#df.head()\n",
        "#Basic information\n",
        "df.info()\n",
        "#Describe the data\n",
        "#df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "## EDA Examples #Find the duplicates\n",
        "## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "#Load the data\n",
        "df = pd.read_csv('/content/sample_data/UNIT3LVADSAIEMPDATAv1.csv')\n",
        "#Find the duplicates\n",
        "df.duplicated().sum() ## Number of duplicate rows in the given data set"
      ],
      "metadata": {
        "id": "ly93YJCsMy5i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1206a364-9df7-4dec-fba4-6aebfdb6c09a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "## EDA Examples #unique values\n",
        "## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "#Load the data\n",
        "df = pd.read_csv('/content/sample_data/DSAILVAORDERDATACSVv1.csv')\n",
        "#unique values\n",
        "\n",
        "#df.describe()\n",
        "df.dtypes\n",
        "\n",
        "#df['CID'].unique() ## Why we are getting this error\n",
        "#df['PID'].unique() ## List Unique Values\n",
        "#df['OID'].unique()\n",
        "#df['ODATE'].unique()\n",
        "#df['OQTY'].unique()\n",
        "#df['OAMOUNT'].unique()\n",
        "#df.isnull().sum() ## In Each Column, How Many Null Values\n",
        "df['PID'].isnull() ## Flase= Not Null, True = Null\n"
      ],
      "metadata": {
        "id": "JSAX338pY8Es"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "## EDA Examples #countplot seaborn\n",
        "## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "\n",
        "#Load the required libraries\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "#Load the data\n",
        "df = pd.read_csv('/content/sample_data/DSAILVAORDERDATACSVv1.csv')\n",
        "sns.countplot(df['OQTY'])"
      ],
      "metadata": {
        "id": "MNCH4NChScBC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "## EDA Examples #Replace null values\n",
        "## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "#Load the data\n",
        "df = pd.read_csv('/content/sample_data/DSAILVAORDERDATACSVv1.csv')\n",
        "#Replace null values\n",
        "df.replace(np.nan,'VAL IS MISSING',inplace = True)\n",
        "#print(df)\n",
        "#Check the changes now\n",
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "LYzbCzA1XQEW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "## EDA Examples ## Filter the Data\n",
        "## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "#Load the data\n",
        "df = pd.read_csv('/content/sample_data/DSAILVAORDERDATACSVv1.csv')\n",
        "#Replace null values\n",
        "df[df['PID']=='Laptop'].head()\n",
        "#df[df['PID']=='Desktop'].head()\n",
        "#df[df['PID']=='Printer'].head()\n"
      ],
      "metadata": {
        "id": "e09TXGy1X9bZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "## EDA Examples ## Boxplot\n",
        "## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "#Load the data\n",
        "df = pd.read_csv('/content/sample_data/DSAILVAORDERDATACSVv1.csv')\n",
        "#Boxplot\n",
        "df[['OAMOUNT']].boxplot()"
      ],
      "metadata": {
        "id": "fjjg2Nx8YvYL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "## EDA Examples #Correlation\n",
        "## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "#Load the data\n",
        "df = pd.read_csv('/content/sample_data/DSAILVAORDERDATACSVv4.csv')\n",
        "df.shape\n",
        "#Correlation\n",
        "df.corr()\n",
        "# This is the correlation matrix with the range from +1 to -1\n",
        "# where +1 is highly and positively correlated and -1 will be highly negatively correlated."
      ],
      "metadata": {
        "id": "eeUYUr-SZmjz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "## EDA Examples #Univariate Analysis\n",
        "## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "# 1. Summary statistics – Measures the center and spread of values.\n",
        "# 2. Frequency table – Describes how often different values occur.\n",
        "# 3. Charts – Used to visualize the distribution of values.\n",
        "## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "#create DataFrame\n",
        "df = pd.DataFrame({'Spend': [1, 1, 2, 3.5, 4, 4, 4, 5, 5, 6.5, 7, 7.4, 8, 13, 14.2],\n",
        "                   'BonusPoints': [5, 7, 7, 9, 12, 9, 9, 4, 6, 8, 8, 9, 3, 2, 6],\n",
        "                   'Discount': [11, 8, 10, 6, 6, 5, 9, 12, 6, 6, 7, 8, 7, 9, 15]})\n",
        "\n",
        "#view first five rows of DataFrame\n",
        "#print(df.head())\n",
        "## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "##1. Calculate Summary Statistics\n",
        "## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "#print('Mean = ', df['Spend'].mean())\n",
        "#print('Median = ', df['Spend'].median())\n",
        "#print('STD = ', df['Spend'].std())\n",
        "## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "#create frequency table for 'points'\n",
        "## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "df['Spend'].value_counts()\n",
        "## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "# Charts – Used to visualize the distribution of values.\n",
        "# Boxplot\n",
        "# Histogram\n",
        "# Density curve\n",
        "## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "#df.boxplot(column=['Spend'], grid=False, color='black')\n",
        "#df.hist(column='Spend', grid=False, edgecolor='black')\n",
        "sns.kdeplot(df['Spend'])"
      ],
      "metadata": {
        "id": "XKH8OCpoDf3b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "## EDA Examples # Bivariate Analysis\n",
        "## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "#  Bivariate analysis refers to the analysis of two variables.\n",
        "#  You can remember this because the prefix “bi” means “two.”\n",
        "# 1. Scatterplots - common ways\n",
        "# 2. Correlation Coefficients - common ways\n",
        "# 3. Simple Linear Regression - common ways\n",
        "## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import statsmodels.api as sm\n",
        "\n",
        "#create DataFrame\n",
        "df = pd.DataFrame({'WORKOUT_HOURS': [1, 1, 1, 2, 2, 2, 3, 3, 3, 3,\n",
        "                             3, 4, 4, 5, 5, 6, 6, 6, 7, 8],\n",
        "                   'CALORIES_BURNED': [75, 66, 68, 74, 78, 72, 85, 82, 90, 82,\n",
        "                             80, 88, 85, 90, 92, 94, 94, 88, 91, 96]})\n",
        "\n",
        "#view first five rows of DataFrame\n",
        "#~~~~~~~~~~~~~~\n",
        "#create scatterplot of WORKOUT_HOURS vs. CALORIES_BURNED\n",
        "#~~~~~~~~~~~~~~\n",
        "#plt.scatter(df.WORKOUT_HOURS, df.CALORIES_BURNED)\n",
        "#plt.title('WORKOUT_HOURS vs. CALORIES_BURNED')\n",
        "#plt.xlabel('WORKOUT_HOURS')\n",
        "#plt.ylabel('CALORIES_BURNED')\n",
        "#~~~~~~~~~~~~~~\n",
        "#create correlation matrix\n",
        "#~~~~~~~~~~~~~~\n",
        "df.corr()\n",
        "## The correlation coefficient is 0.891.\n",
        "## This indicates a strong positive correlation between WORKOUT_HOURS and CALORIES_BURNED.\n",
        "#~~~~~~~~~~~~~~\n",
        "# Simple Linear Regression\n",
        "# A statistical method we can use to quantify the relationship between two variables.\n",
        "#~~~~~~~~~~~~~~\n",
        "#define response variable # Output\n",
        "y = df['CALORIES_BURNED']\n",
        "#define explanatory variable # Input\n",
        "x = df[['WORKOUT_HOURS']]\n",
        "#add constant to predictor variables\n",
        "x = sm.add_constant(x)\n",
        "#fit linear regression model\n",
        "model = sm.OLS(y, x).fit()\n",
        "#view model summary\n",
        "print(model.summary())\n",
        "#~~~~~~~~~~~~~~\n",
        "#CALORIES_BURNED = 69.0734 + 3.8471*(WORKOUT_HOURS)\n",
        "# Each additional WORKOUT_HOURS is associated with an average increase of 3.8471 CALORIES_BURNED.\n",
        "#~~~~~~~~~~~~~~"
      ],
      "metadata": {
        "id": "MDKO3S7gKEuk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc1582fa-db2c-471c-9db5-daae3fd49529"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                            OLS Regression Results                            \n",
            "==============================================================================\n",
            "Dep. Variable:        CALORIES_BURNED   R-squared:                       0.794\n",
            "Model:                            OLS   Adj. R-squared:                  0.783\n",
            "Method:                 Least Squares   F-statistic:                     69.56\n",
            "Date:                Mon, 18 Mar 2024   Prob (F-statistic):           1.35e-07\n",
            "Time:                        06:57:34   Log-Likelihood:                -55.886\n",
            "No. Observations:                  20   AIC:                             115.8\n",
            "Df Residuals:                      18   BIC:                             117.8\n",
            "Df Model:                           1                                         \n",
            "Covariance Type:            nonrobust                                         \n",
            "=================================================================================\n",
            "                    coef    std err          t      P>|t|      [0.025      0.975]\n",
            "---------------------------------------------------------------------------------\n",
            "const            69.0734      1.965     35.149      0.000      64.945      73.202\n",
            "WORKOUT_HOURS     3.8471      0.461      8.340      0.000       2.878       4.816\n",
            "==============================================================================\n",
            "Omnibus:                        0.171   Durbin-Watson:                   1.404\n",
            "Prob(Omnibus):                  0.918   Jarque-Bera (JB):                0.177\n",
            "Skew:                           0.165   Prob(JB):                        0.915\n",
            "Kurtosis:                       2.679   Cond. No.                         9.37\n",
            "==============================================================================\n",
            "\n",
            "Notes:\n",
            "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "## EDA Examples #Correlation plot\n",
        "## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "#Load the data\n",
        "df = pd.read_csv('/content/sample_data/DSAILVAORDERDATACSVv2.csv')\n",
        "df.shape\n",
        "#Correlation plot\n",
        "#sns.heatmap(df.corr())"
      ],
      "metadata": {
        "id": "7y8UFRz2Z6y4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "## EDA Examples #multivariate analysis using a correlation matrix plot.\n",
        "## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings as wr\n",
        "wr.filterwarnings('ignore')\n",
        "\n",
        "#Load the data\n",
        "df = pd.read_csv('/content/sample_data/DSAILVAORDERDATACSVv3.csv')\n",
        "\n",
        "# Assuming 'df' is your DataFrame\n",
        "plt.figure(figsize=(15, 10))\n",
        "\n",
        "# Using Seaborn to create a heatmap\n",
        "sns.heatmap(df.corr(), annot=True, fmt='.4f', cmap='Pastel2', linewidths=4)\n",
        "\n",
        "plt.title('Correlation Heatmap')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "b6yD40Ixb8b4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "## EDA Examples #multivariate analysis using a correlation matrix plot.\n",
        "## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings as wr\n",
        "wr.filterwarnings('ignore')\n",
        "\n",
        "#Load the data\n",
        "df = pd.read_csv('/content/sample_data/DSAILVAORDERDATACSVv2.csv')\n",
        "#plotting box plot between alcohol and quality\n",
        "sns.boxplot(x='OID', y='OQTY', data=df)"
      ],
      "metadata": {
        "id": "WNkqqed-efPC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}